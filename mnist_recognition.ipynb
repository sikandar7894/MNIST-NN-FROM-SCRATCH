{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from array import array\n",
    "from os.path  import join\n",
    "import matplotlib.pyplot as plt\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "input_path = '/Users/parasmalik/Documents/Assignment_SigTuple/'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte')\n",
    "\n",
    "# Load MINST dataset\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of testing examples: 10000\n",
      "Each image is of size: (28, 28, 1)\n",
      "train_x_orig shape: (60000, 28, 28)\n",
      "train_y shape: (60000,)\n",
      "test_x_orig shape: (10000, 28, 28)\n",
      "test_y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(x_train)\n",
    "Y_train = np.array(y_train)\n",
    "X_test = np.array(x_test)\n",
    "Y_test = np.array(y_test)\n",
    "\n",
    "m_train = X_train.shape[0]\n",
    "num_px = X_train.shape[1]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "# Printing the dataset values\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 1)\")\n",
    "print (\"train_x_orig shape: \" + str(X_train.shape))\n",
    "print (\"train_y shape: \" + str(Y_train.shape))\n",
    "print (\"test_x_orig shape: \" + str(X_test.shape))\n",
    "print (\"test_y shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the imgaes \n",
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1).T \n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1\n",
    "train_X = X_train_flatten / 255.\n",
    "test_X = X_test_flatten / 255.\n",
    "\n",
    "# one-hot encoding of labesls for back propagating\n",
    "digits = 10\n",
    "train_Y = np.eye(digits)[Y_train.astype('int32')]\n",
    "train_Y = train_Y.T.reshape(digits, 60000)\n",
    "test_Y = np.eye(digits)[Y_test.astype('int32')]\n",
    "test_Y = test_Y.T.reshape(digits, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Hyperparameters\n",
    "\"\"\"\n",
    "After performing the experiment on various architect I got the best result with 2 dimensional Neural Network where \n",
    "the 1st activation function is relu\n",
    "\"\"\"\n",
    "n_x = 784     \n",
    "n_h = 64\n",
    "n_y = 10\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "lr = 0.03 #learning rate\n",
    "beta= 0.9\n",
    "epochs = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))  \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "params = initialize_parameters(n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    s = 1. / (1. + np.exp(-Z))\n",
    "    return s\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    return A\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(AL, Y):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    logloss = np.multiply(np.log(AL), Y) + np.multiply((1 - Y), np.log(1 - AL))\n",
    "    cost = -1/m*np.sum(logloss)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    \n",
    "    cache = {}\n",
    "    # Z1 = W1.dot(x) + b1\n",
    "    cache[\"Z1\"] = (params[\"W1\"]@ X) + params[\"b1\"]\n",
    "    # A1 = relu(Z1)\n",
    "    cache[\"A1\"] = relu(cache[\"Z1\"])\n",
    "    # Z2 = W2.dot(A1) + b2\n",
    "    cache[\"Z2\"] = (params[\"W2\"]@cache[\"A1\"]) + params[\"b2\"]\n",
    "    # A2 = softmax(Z2)\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "    return cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(X, Y, params, cache, m_batch):\n",
    "    \n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = (1. / m_batch) * (dZ2@cache[\"A1\"].T)\n",
    "    db2 = (1. / m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = params[\"W2\"].T@ dZ2\n",
    "    dZ1 = relu_backward(dA1,cache[\"Z1\"])\n",
    "    dW1 = (1. / m_batch) * (dZ1@X.T)\n",
    "    db1 = (1. / m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 0.74867365893571\n",
      "Accuracy on test set is 87.66000000000001 %\n",
      "Epoch 2: training loss = 0.5743661616985307\n",
      "Accuracy on test set is 90.39 %\n",
      "Epoch 3: training loss = 0.5101266841701566\n",
      "Accuracy on test set is 91.55 %\n",
      "Epoch 4: training loss = 0.46518059683033613\n",
      "Accuracy on test set is 92.31 %\n",
      "Epoch 5: training loss = 0.42563130157463136\n",
      "Accuracy on test set is 92.93 %\n",
      "Epoch 6: training loss = 0.3909075107194864\n",
      "Accuracy on test set is 93.42 %\n",
      "Epoch 7: training loss = 0.3612496098991478\n",
      "Accuracy on test set is 93.89999999999999 %\n",
      "Epoch 8: training loss = 0.3364471261077634\n",
      "Accuracy on test set is 94.28 %\n",
      "Epoch 9: training loss = 0.31480006413338674\n",
      "Accuracy on test set is 94.67 %\n",
      "Epoch 10: training loss = 0.29609097544700586\n",
      "Accuracy on test set is 94.89 %\n",
      "Epoch 11: training loss = 0.2795115467340682\n",
      "Accuracy on test set is 95.09 %\n",
      "Epoch 12: training loss = 0.26476064824327367\n",
      "Accuracy on test set is 95.34 %\n",
      "Epoch 13: training loss = 0.2513732867757008\n",
      "Accuracy on test set is 95.61 %\n",
      "Epoch 14: training loss = 0.2391573610863826\n",
      "Accuracy on test set is 95.69 %\n",
      "Epoch 15: training loss = 0.228275429339067\n",
      "Accuracy on test set is 95.8 %\n",
      "Epoch 16: training loss = 0.2181230107944009\n",
      "Accuracy on test set is 95.89999999999999 %\n",
      "Epoch 17: training loss = 0.20889249518781774\n",
      "Accuracy on test set is 96.11 %\n",
      "Epoch 18: training loss = 0.2006168447615933\n",
      "Accuracy on test set is 96.3 %\n",
      "Epoch 19: training loss = 0.19289994625331774\n",
      "Accuracy on test set is 96.43 %\n",
      "Epoch 20: training loss = 0.18586973713886748\n",
      "Accuracy on test set is 96.44 %\n",
      "Epoch 21: training loss = 0.1790984438969789\n",
      "Accuracy on test set is 96.50999999999999 %\n",
      "Epoch 22: training loss = 0.173005437824064\n",
      "Accuracy on test set is 96.54 %\n",
      "Epoch 23: training loss = 0.16734489245686382\n",
      "Accuracy on test set is 96.61999999999999 %\n",
      "Epoch 24: training loss = 0.16200632880899543\n",
      "Accuracy on test set is 96.71 %\n",
      "Epoch 25: training loss = 0.15696890883584325\n",
      "Accuracy on test set is 96.83 %\n",
      "Epoch 26: training loss = 0.15229451925474555\n",
      "Accuracy on test set is 96.89 %\n",
      "Epoch 27: training loss = 0.14792623547165526\n",
      "Accuracy on test set is 96.89999999999999 %\n",
      "Epoch 28: training loss = 0.14371624752242612\n",
      "Accuracy on test set is 96.95 %\n",
      "Epoch 29: training loss = 0.13962016776027658\n",
      "Accuracy on test set is 97.00999999999999 %\n",
      "Epoch 30: training loss = 0.13580796595793249\n",
      "Accuracy on test set is 97.06 %\n",
      "Epoch 31: training loss = 0.1321462134352402\n",
      "Accuracy on test set is 97.1 %\n",
      "Epoch 32: training loss = 0.1286725394106441\n",
      "Accuracy on test set is 97.15 %\n",
      "Epoch 33: training loss = 0.12531359912006687\n",
      "Accuracy on test set is 97.2 %\n",
      "Epoch 34: training loss = 0.12207561585195346\n",
      "Accuracy on test set is 97.21 %\n",
      "Epoch 35: training loss = 0.118930586444467\n",
      "Accuracy on test set is 97.22 %\n",
      "Epoch 36: training loss = 0.11596903452452156\n",
      "Accuracy on test set is 97.27 %\n",
      "Epoch 37: training loss = 0.11315340591133442\n",
      "Accuracy on test set is 97.27 %\n",
      "Epoch 38: training loss = 0.1104415306857976\n",
      "Accuracy on test set is 97.3 %\n",
      "Epoch 39: training loss = 0.10769761317558103\n",
      "Accuracy on test set is 97.28999999999999 %\n",
      "Epoch 40: training loss = 0.10508620341896283\n",
      "Accuracy on test set is 97.27 %\n",
      "Epoch 41: training loss = 0.10264722956854405\n",
      "Accuracy on test set is 97.3 %\n",
      "Epoch 42: training loss = 0.10019378058127089\n",
      "Accuracy on test set is 97.33000000000001 %\n",
      "Epoch 43: training loss = 0.09788151016873896\n",
      "Accuracy on test set is 97.34 %\n",
      "Epoch 44: training loss = 0.09568786672911611\n",
      "Accuracy on test set is 97.39 %\n",
      "Epoch 45: training loss = 0.09361112469605115\n",
      "Accuracy on test set is 97.39999999999999 %\n",
      "Epoch 46: training loss = 0.09167638352614646\n",
      "Accuracy on test set is 97.39999999999999 %\n",
      "Epoch 47: training loss = 0.08969779519844645\n",
      "Accuracy on test set is 97.38 %\n",
      "Epoch 48: training loss = 0.08782634731356863\n",
      "Accuracy on test set is 97.39999999999999 %\n",
      "Epoch 49: training loss = 0.08597473762213725\n",
      "Accuracy on test set is 97.41 %\n",
      "Epoch 50: training loss = 0.08422957758013598\n",
      "Accuracy on test set is 97.42 %\n",
      "Epoch 51: training loss = 0.0824642817358212\n",
      "Accuracy on test set is 97.46000000000001 %\n",
      "Epoch 52: training loss = 0.08072991639093023\n",
      "Accuracy on test set is 97.43 %\n",
      "Epoch 53: training loss = 0.07912483137471714\n",
      "Accuracy on test set is 97.44 %\n",
      "Epoch 54: training loss = 0.07755302516857929\n",
      "Accuracy on test set is 97.45 %\n",
      "Epoch 55: training loss = 0.07599661419998456\n",
      "Accuracy on test set is 97.48 %\n",
      "Epoch 56: training loss = 0.07452115244528958\n",
      "Accuracy on test set is 97.5 %\n",
      "Epoch 57: training loss = 0.07301767257374965\n",
      "Accuracy on test set is 97.52 %\n",
      "Epoch 58: training loss = 0.07158188336780212\n",
      "Accuracy on test set is 97.50999999999999 %\n",
      "Epoch 59: training loss = 0.07025809616924997\n",
      "Accuracy on test set is 97.52 %\n",
      "Epoch 60: training loss = 0.06893339847093269\n",
      "Accuracy on test set is 97.53 %\n",
      "Epoch 61: training loss = 0.067710494726673\n",
      "Accuracy on test set is 97.53 %\n",
      "Epoch 62: training loss = 0.06640694913497204\n",
      "Accuracy on test set is 97.50999999999999 %\n",
      "Epoch 63: training loss = 0.06517829195886972\n",
      "Accuracy on test set is 97.50999999999999 %\n",
      "Epoch 64: training loss = 0.06397126650563184\n",
      "Accuracy on test set is 97.52 %\n",
      "Epoch 65: training loss = 0.06278967570184438\n",
      "Accuracy on test set is 97.52 %\n",
      "Epoch 66: training loss = 0.061662276088671815\n",
      "Accuracy on test set is 97.53 %\n",
      "Epoch 67: training loss = 0.06057021850673329\n",
      "Accuracy on test set is 97.54 %\n",
      "Epoch 68: training loss = 0.05948912955096967\n",
      "Accuracy on test set is 97.55 %\n",
      "Epoch 69: training loss = 0.05842554973413558\n",
      "Accuracy on test set is 97.56 %\n",
      "Epoch 70: training loss = 0.057405471062208085\n",
      "Accuracy on test set is 97.55 %\n",
      "Epoch 71: training loss = 0.056369306918796686\n",
      "Accuracy on test set is 97.58 %\n",
      "Epoch 72: training loss = 0.055431640262080875\n",
      "Accuracy on test set is 97.6 %\n",
      "Epoch 73: training loss = 0.05439915837336789\n",
      "Accuracy on test set is 97.61 %\n",
      "Epoch 74: training loss = 0.053494127552147074\n",
      "Accuracy on test set is 97.64 %\n",
      "Epoch 75: training loss = 0.052555750062224\n",
      "Accuracy on test set is 97.65 %\n",
      "Epoch 76: training loss = 0.05166968488608828\n",
      "Accuracy on test set is 97.68 %\n",
      "Epoch 77: training loss = 0.05076498734324767\n",
      "Accuracy on test set is 97.68 %\n",
      "Epoch 78: training loss = 0.04988763228012317\n",
      "Accuracy on test set is 97.69 %\n",
      "Epoch 79: training loss = 0.04913965973327652\n",
      "Accuracy on test set is 97.68 %\n",
      "Epoch 80: training loss = 0.04832234183868201\n",
      "Accuracy on test set is 97.66 %\n",
      "Epoch 81: training loss = 0.04748110174228942\n",
      "Accuracy on test set is 97.66 %\n",
      "Epoch 82: training loss = 0.046725030705111234\n",
      "Accuracy on test set is 97.7 %\n",
      "Epoch 83: training loss = 0.04590223775090342\n",
      "Accuracy on test set is 97.69 %\n",
      "Epoch 84: training loss = 0.04520234531055656\n",
      "Accuracy on test set is 97.69 %\n",
      "Epoch 85: training loss = 0.04442135270771958\n",
      "Accuracy on test set is 97.68 %\n",
      "Epoch 86: training loss = 0.04370074027494995\n",
      "Accuracy on test set is 97.7 %\n",
      "Epoch 87: training loss = 0.043015905759798224\n",
      "Accuracy on test set is 97.69 %\n",
      "Epoch 88: training loss = 0.04226317312698065\n",
      "Accuracy on test set is 97.7 %\n",
      "Epoch 89: training loss = 0.04164153753094643\n",
      "Accuracy on test set is 97.7 %\n",
      "Epoch 90: training loss = 0.04099187611779097\n",
      "Accuracy on test set is 97.68 %\n",
      "Epoch 91: training loss = 0.04030582313172545\n",
      "Accuracy on test set is 97.72 %\n",
      "Epoch 92: training loss = 0.0396606062857243\n",
      "Accuracy on test set is 97.7 %\n",
      "Epoch 93: training loss = 0.039033820376697004\n",
      "Accuracy on test set is 97.69 %\n",
      "Epoch 94: training loss = 0.038490324775460175\n",
      "Accuracy on test set is 97.72 %\n",
      "Epoch 95: training loss = 0.037812695400214806\n",
      "Accuracy on test set is 97.72999999999999 %\n",
      "Epoch 96: training loss = 0.037241362039439944\n",
      "Accuracy on test set is 97.74000000000001 %\n",
      "Epoch 97: training loss = 0.03666476073251594\n",
      "Accuracy on test set is 97.74000000000001 %\n",
      "Epoch 98: training loss = 0.036094260463285927\n",
      "Accuracy on test set is 97.72 %\n",
      "Epoch 99: training loss = 0.035610516064321224\n",
      "Accuracy on test set is 97.75 %\n",
      "Epoch 100: training loss = 0.03504322751531072\n",
      "Accuracy on test set is 97.72999999999999 %\n"
     ]
    }
   ],
   "source": [
    "batches = 60000//batch_size\n",
    "#Training on 100 epochs with weight updates happed after each bacth of 64 examples\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for j in range(batches):\n",
    "        \n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, 60000 - 1)\n",
    "        X = train_X[:, begin:end]\n",
    "        Y = train_Y[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        # forward and backward\n",
    "        cache = forward_propagation(X, params)\n",
    "        grads = back_propagate(X, Y, params, cache, m_batch)\n",
    "        \n",
    "        # Finding partial derivatives of loss w.r.t. weights\n",
    "        dW1 = grads[\"dW1\"]\n",
    "        db1 = grads[\"db1\"]\n",
    "        dW2 = grads[\"dW2\"]\n",
    "        db2 = grads[\"db2\"]\n",
    "        dW1 = (beta * dW1 + (1. - beta) * grads[\"dW1\"])\n",
    "        db1 = (beta * db1 + (1. - beta) * grads[\"db1\"])\n",
    "        dW2 = (beta * dW2 + (1. - beta) * grads[\"dW2\"])\n",
    "        db2 = (beta * db2 + (1. - beta) * grads[\"db2\"])\n",
    "\n",
    "        # gradient descent and updating weights\n",
    "        params[\"W1\"] = params[\"W1\"] - lr * dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - lr * db1\n",
    "        params[\"W2\"] = params[\"W2\"] - lr * dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - lr * db2\n",
    "\n",
    "    cache = forward_propagation(train_X, params)\n",
    "    train_loss = compute_loss( cache[\"A2\"],train_Y)\n",
    "\n",
    "    cache = forward_propagation(test_X, params)\n",
    "    test_loss = compute_loss( cache[\"A2\"],test_Y)\n",
    "    print(\"Epoch {}: training loss = {}\".format(\n",
    "        i + 1, train_loss))\n",
    "    \n",
    "    test_prediction = forward_propagation(test_X, params)\n",
    "    Y_predicted = np.argmax(test_prediction[\"A2\"],axis = 0)\n",
    "    result = np.sum(np.where(Y_predicted == Y_test, 1,0))\n",
    "\n",
    "    Accuracy = result/m_test \n",
    "\n",
    "    print(\"Accuracy on test set is {} %\".format(100*Accuracy))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 97.72999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# prediction of All Test Imagees\n",
    "#print(np.argmax(test_prediction[\"A2\"],axis = 0))\n",
    "#print(Y_test)\n",
    "\n",
    "Y_predicted = np.argmax(test_prediction[\"A2\"],axis = 0)\n",
    "result = np.sum(np.where(Y_predicted == Y_test, 1,0))\n",
    "\n",
    "Accuracy = result/m_test \n",
    "\n",
    "print(\"Accuracy on test set is\", 100*Accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select any random image index 203\n",
      "Actual number 1\n",
      "Predicted value through our model\n",
      "[1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Picking any random image and checking the output\n",
    "\n",
    "k = int(input(\"Select any random image index \"))\n",
    "print(\"Actual number\",Y_test[k])\n",
    "plt.imshow(x_test[k], cmap=plt.cm.gray)\n",
    "test_X_1 = test_X.T\n",
    "test_X_1 = test_X_1[k]\n",
    "test_X_1 = test_X_1.reshape(784,-1)\n",
    "predict = forward_propagation(test_X_1,params)\n",
    "print(\"Predicted value through our model\")\n",
    "print(np.argmax(predict[\"A2\"],axis = 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
